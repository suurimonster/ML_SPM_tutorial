{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c413a97-9579-4950-9166-1d80256b629b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Machine learning and microscopy\n",
    "=====================\n",
    "\n",
    "Moving to machine learning\n",
    "------------\n",
    "\n",
    "* Machine learning at heart is a software toolset to apply statistical methods to find patterns in data.\n",
    "* “The ability [of machine learning] to automatically identify patterns in data [...] is particularly important when the expert knowledge is incomplete or inaccurate, when the amount of available data is too large to be handled manually, or when there are exceptions to the general cases” [1]\n",
    "\n",
    "![ML](images/machine_learning_joke.jpg)\n",
    "\n",
    "Why NOT use maching learning?\n",
    "------------\n",
    "\n",
    "* It is very tempting to start applying machine learning approaches to everything, but it is critical to step back and see if it makes sense.\n",
    "* Some major reasons NOT to apply an machine learning approach would be:\n",
    "    * There are existing methods that do the job well - there is no point inventing a square wheel.\n",
    "    * You don't have enough data - in general you want to have a ratio of 10:1 between *datapoints* and *features*.\n",
    "    * Your data is poorly organized or heavily biased - if you are not sure about the history and compatibility of your data you will not be able to trust any of your predictions.\n",
    "\n",
    "[1]: Yip KY, Cheng C, Gerstein M. Machine learning and genome annotation: a match meant to be? Genome Biol. 2013;14(5):205."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c56d9-801a-4c7c-95ba-2b35e3701a89",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Recent examples\n",
    "====================\n",
    "\n",
    "Image segmentation\n",
    "---------------------\n",
    "\n",
    "* A very common application of machine learning in microscopy is image segmentation i.e. automatically detecting features in images.\n",
    "* This is generally achieved using Convolutional Neural Networks (CNN) and the approach is the backbone for all the animal classification methods you have likely seen already.\n",
    "* This classification is an example of *supervised learning*, where the training data is labelled by the user. The algorithm then learns this classification and is able to make accurate predictions on new data.\n",
    "* For an example, we use the U-Net method[1], which is a powerful approach, widely used in many CNN applications. For biomedial imaging, they actually broke some of our inital rules...they had very little data, but used extensive *augmentation* to expand the valid data set. In particular (and general to microscopy of soft systems) they introduced:\n",
    "    * shift and rotation invariance \n",
    "    * gray value variations\n",
    "    * random elastic deformations\n",
    "* It is not perfect (77%), but much better than any alternatives at that time.\n",
    "\n",
    "![segment](images/segment.png)\n",
    "\n",
    "[1]: Ronneberger, O., Fischer, P. & Brox, T. U-Net: Convolutional Networks for Biomedical Image Segmentation. arXiv:1505.04597 [cs] (2015).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe9be1d-3a3b-4229-9fce-95bb8b7c5b84",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Cell classification\n",
    "---------------------\n",
    "\n",
    "* In terms of searching for patterns in images that demonstrate a correlation between a measurement and a hypothesis, machine learning image analysis can also be very powerful.\n",
    "* In the example, they looked at multi-channel AFM data of healthy and cancerous (fixed) cells [1].\n",
    "\n",
    "![cancer_cells](images/cancer_cells.png)\n",
    "\n",
    "* Model was trained on the imaging channels independently and in combination, and the resultant classification accuracy was studied as a function of measured parameters. Very high accuracy was achieved using a combination of channels.\n",
    "\n",
    "![cancer_acc](images/cancer_acc.png)\n",
    "\n",
    "[1]: Prasad, S. et al. Atomic Force Microscopy Detects the Difference in Cancer Cells of Different Neoplastic Aggressiveness via Machine Learning. Advanced NanoBiomed Research 1, 2000116 (2021).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170fe77b-e966-4209-8c64-76da6401f9dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Active learning\n",
    "--------------------\n",
    "\n",
    "* A powerful development in experimental design is the introduction of active learning (also referred to as *Bayesian Optimisation*), where measurements are dynamically chosen to optimise the information obtained.\n",
    "* In piezoresponse force microscopy, this has been used to automate the measurement of spectra at regions of specific interest [1]. The system is first trained on known surface features and spectra.\n",
    "\n",
    "![active_model](images/active_model.png)\n",
    "\n",
    "* Then, when it is linked to the driving software of an SPM, the network itself can make a decision about the most useful place to measure the spectra. These *decisions* can then actually tell us something about the correlation between image features and the nature of the spectra.\n",
    "\n",
    "![active_image](images/active_image.png)\n",
    "\n",
    "[1]: Liu, Y., Kelley, K.P., Vasudevan, R.K. et al. Experimental discovery of structure–property relationships in ferroelectric materials via active learning. Nat Mach Intell 4, 341–350 (2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44bd44f-e736-45cc-b831-29115336af33",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Machine learning methods\n",
    "============================\n",
    "\n",
    "* There are a wide variety of machine learning approaches and as a general rule it is better to use the simplest one that works. In microscopy, since we are often working with images, then Neural Networks (NN) are an obvious place to start, as there has been an enormous effort in recent years to optimise NN tools for image analysis. They also play role in all the examples/tutorials we are considering. Do NOT use them by default...\n",
    "\n",
    "Neural Networks\n",
    "-------------------\n",
    "\n",
    "* The starting point of a neural network is the neuron itself (called a *perceptron* in the context of machine learning) - it takes inputs, applies some operations to them and produces an output [1].\n",
    "\n",
    "![neuron](images/perceptron.svg)\n",
    "\n",
    "* First each input $x_1,x_2$ is multiplied by weights $w_1,w_2$. Then all the weighted inputs are added together with a bias $b$ and an activation function $f$ is applied to the total:\n",
    "\n",
    "$y = f((w_1*x_1)+(w_2*x_2) + b$)\n",
    "\n",
    "* The bias reflects your pre-assumptions in the model i.e. the higher it is, the more you think you know what the answer should be, and the weights determine the strength of connection between this input and the output. These are the *hyper-parameters* that will be optimized.\n",
    "\n",
    "* The activation function turns an unbounded input into a controllable, bounded input. A commonly used example would be a sigmoid function.\n",
    "\n",
    "[1]: Neural Networks from Scratch - https://victorzhou.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91801177-14ab-46b9-afd7-41add675977f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "\n",
    "x = np.linspace(-10, 10, 100) \n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "a = sigmoid(x)\n",
    "\n",
    "plt.figure(figsize=(12,7)) # set the figsize\n",
    "plt.plot(x, a) \n",
    "plt.xlabel(\"x\") \n",
    "plt.ylabel(\"sigmoid(x)\")\n",
    "plt.rc('font',size=16) # set the font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e02295-a49d-4e04-83e3-01ff4ae08604",
   "metadata": {},
   "source": [
    "* We can then code a very simple neuron as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ab0ba1-0759-441e-9df8-8b4e8c00016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Neuron: # classes are a powerful aspect of python that allows inherited characteristics\n",
    "  def __init__(self, weights, bias):\n",
    "    self.weights = weights\n",
    "    self.bias = bias\n",
    "\n",
    "  def feedforward(self, inputs): # passing inputs to get an output is known as \"feed forward\"\n",
    "    # Weight inputs, add bias, then use the activation function\n",
    "    total = np.dot(self.weights, inputs) + self.bias\n",
    "    return sigmoid(total)\n",
    "\n",
    "weights = np.array([0, 1]) # w1 = 0, w2 = 1\n",
    "bias = 4                   # b = 4\n",
    "n = Neuron(weights, bias)\n",
    "\n",
    "x = np.array([2, 3])       # x1 = 2, x2 = 3\n",
    "print(n.feedforward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07d2020-dfc2-4a06-8699-b1bca6bcc627",
   "metadata": {},
   "source": [
    "* A neural network is just formed by combing neurons and connecting them.\n",
    "\n",
    "![nn](images/network.svg)\n",
    "\n",
    "* Again, we can make a simple code that implements this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f337e5fa-4886-4af1-838d-1be081ad3c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Neural_Network:\n",
    "  '''\n",
    "  A neural network with:\n",
    "    - 2 inputs\n",
    "    - a hidden layer with 2 neurons (h1, h2)\n",
    "    - an output layer with 1 neuron (o1)\n",
    "  Each neuron has the same weights and bias:\n",
    "    - w = [0, 1]\n",
    "    - b = 0\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    weights = np.array([0, 1])\n",
    "    bias = 0\n",
    "\n",
    "    # The Neuron class here is from the previous code sample\n",
    "    self.h1 = Neuron(weights, bias)\n",
    "    self.h2 = Neuron(weights, bias)\n",
    "    self.o1 = Neuron(weights, bias)\n",
    "\n",
    "  def feedforward(self, x):\n",
    "    out_h1 = self.h1.feedforward(x)\n",
    "    out_h2 = self.h2.feedforward(x)\n",
    "\n",
    "    # The inputs for o1 are the outputs from h1 and h2\n",
    "    out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "\n",
    "    return out_o1\n",
    "\n",
    "network = Neural_Network()\n",
    "x = np.array([2, 3])\n",
    "print(network.feedforward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f73699-34b6-45e5-95c5-5e2d04da31b0",
   "metadata": {},
   "source": [
    "* This appears very abstract, but if we were classifying articles as either from physics or biology based on the number of equations in them, we might assign an output of (0) to physics and (1) to biology, and then train the network on data on equation numbers per article from each field. The training would attempt to minimize the *loss* in the output with respect to the parameters of the model.\n",
    "* The number of hidden layers (the layers between input and output) determine the *depth* of the network and is the source of the term *deep learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4909e6e3-20d6-4b51-94ac-a7e5947d4f52",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Tutorial 1\n",
    "\n",
    "### Iris flower data set\n",
    "\n",
    "The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper *\"The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis\"*.\n",
    "\n",
    "The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a032139a-120e-4cba-9829-2a134136ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "with open(\"examples/data/iris.pkl\", \"rb\") as fio:\n",
    "    iris = pickle.load(fio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a01611-eafc-4128-9c2a-ef1149b0abb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see the data\n",
    "from pprint import pprint\n",
    "pprint(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f8c62-51a5-4cae-9223-a151578a386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data\n",
    "x = iris[\"data\"]\n",
    "input_shape = (4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d9fe57-1dbb-450a-a68e-0ab2911dca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output data\n",
    "target = iris[\"target\"]\n",
    "num_classes = 3\n",
    "# one-hot representation\n",
    "y = keras.utils.to_categorical(target, num_classes)\n",
    "\n",
    "print(f\"targets: {target[:5]}\\none-hot rep:{y[:5, :]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5561ad8-9c15-4b2d-90f7-68da24dc4ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "shuffle_indices = np.random.permutation(np.arange(len(target)))\n",
    "x = x[shuffle_indices]\n",
    "y = y[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ddad2a-760c-435d-ac5e-2821621ee84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_train split 80-20\n",
    "split = int(len(target) * 0.8)\n",
    "x_train, x_test, y_train, y_test = x[:split, :], x[split:, :], y[:split, :], y[split:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d2652-30b4-4065-b167-5d91821f3678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Dense(8, activation=\"relu\"),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0783db-8763-42d9-b92f-99680ede9961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "batch_size = 5 # number of images in each batch used for optimisation - balance speed of each epoch against convergence to high accuracy\n",
    "epochs = 200\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd386c26-91fc-4bcf-ace6-0758140cb844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b72472-5432-4471-b25c-eeabfaeac95d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Convolutional neural networks\n",
    "----------------------------------\n",
    "\n",
    "* Nearly all image analysis relies on using Convolutional neural networks (CNN) rather than standard neural networks. This two main reasons:\n",
    "    * Images are big - imagine building a neural network to process a 224x224 color image - including the 3 color channels (RGB) in the image, that comes out to 224 x 224 x 3 = 150 528 input features. A typical hidden layer in such a network might have 1024 nodes, so we’d have to train 150 528 x 1024 = 150+ million weights for the first layer alone. Our network would be huge and nearly impossible to train.\n",
    "    * Positions can change - if you trained a network to detect dogs, you’d want it to be able to a detect a dog regardless of where it appears in the image. Imagine training a network that works well on a certain dog image, but then feeding it a slightly shifted version of the same image. The dog would not activate the same neurons, so the network would react completely differently.\n",
    "* What we really want to do is learn characteristic features within our training images that would be general to the subject of interest rather than the image itself.\n",
    "\n",
    "*Convolutions*\n",
    "\n",
    "* A CNN basically convolves the image with a filter that will extract features. For example, the Sobel filter:\n",
    "\n",
    "![sobel](images/vertical-sobel.svg)\n",
    "\n",
    "when applied to a simple greyscale image, convolves it into a smaller matrix (using *padding* i.e. adding zeroes at the edges, would allow us to sample more pixels):\n",
    "\n",
    "![convolve](images/convolve-output.gif)\n",
    "\n",
    "at each step it is just performing an element-wise multiplication between the values in the filter and their corresponding values in the image, and then summing up all the element-wise products to get the new output.\n",
    "\n",
    "* What does this actually do?\n",
    "\n",
    "![lenna](images/lenna+vertical.png)\n",
    "\n",
    "The Sobel filter is an edge detector - the image has been transformed into a set of *edge features*.\n",
    "\n",
    "*Pooling*\n",
    "\n",
    "* It is obvious to us (but not to the *machine*) that many features are connected such that many neighbouring pixels have similar values. In the context of the edge detection filter, if we find a strong edge a location, it is pretty likely that the edge will also be present one pixel away.\n",
    "\n",
    "* To avoid duplicating features, this is normally handled by using *pooling* of neighbouring pixels to there maximum, minimum or average (max-, min- and mean-pooling).\n",
    "\n",
    "![pool](images/pool.gif)\n",
    "\n",
    "*Softmax*\n",
    "\n",
    "* Finally, we need an activation function, just as in the standard NN. A commonly used function for CNNs is Softmax, which has the following form:\n",
    "\n",
    "${Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$\n",
    "\n",
    "* Softmax produces a value from 0 to 1, as for the Sigmoid, but also the outputs add up to 1, so it has the form of a probability - this is not needed, but it allows for the use of *cross entropy loss*, which includes a component of prediction confidence. So the very basic form of a CNN is the following:\n",
    "\n",
    "![cnn](images/cnn.svg)\n",
    "\n",
    "* Here we start with a 28x28 image and convolve it with 8 filters. Using a maximum pooling with size 2 reduces the image to 13x13. There are 10 Softmax nodes, which would assume we are trying to classify our data into 10 classes. The final output from Softmax for a given image would be the node with the highest probability i.e. the class prediction.\n",
    "\n",
    "*Training*\n",
    "\n",
    "* Before we actually let you play around with training your own network, a few comments the process itself. It generally consists of two phases:\n",
    "    * A forward phase, where the input is passed completely through the network - key values are stored in preparation for the backwards pass.\n",
    "    * A backward phase, where gradients are backpropagated (backprop) and weights are updated. During this phase, each layer will receive the gradient of loss with respect to its outputs and return the gradient of loss with respect to its inputs - this establishes the dependence of loss on our parameters and allows for optimisation (with a standard optimiser like ADAM).\n",
    "* The basic code for CNN training would have this form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf0afe7-2185-4f2c-b652-b99a4042c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed forward\n",
    "out = conv.forward(image)\n",
    "out = pool.forward(out)\n",
    "out = softmax.forward(out)\n",
    "\n",
    "# Calculate initial gradient\n",
    "gradient = np.zeros(10)\n",
    "# ...\n",
    "\n",
    "# Backprop\n",
    "gradient = softmax.backprop(gradient)\n",
    "gradient = pool.backprop(gradient)\n",
    "gradient = conv.backprop(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad94fb9-baea-4d09-b102-cf90bca2eee7",
   "metadata": {},
   "source": [
    "* The core of training is in the `conv.backprop` step, as any changes in the filter weights is effectively changing the nature of the filters and the type of features associated with them.\n",
    "* When really applying these methods in a scientific context, it is much more efficient to use highly optimised models, such as `tensorflow`. As in tutorial 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71612c5-2c32-45c4-90a1-217f53143185",
   "metadata": {},
   "source": [
    "## Example \n",
    "=====================\n",
    "\n",
    "### CO functionalised AFM tip classification\n",
    "An automated solution for carbon monoxide functionalization which combines machine learning descriptors with automated software control of the tip preparation process. \n",
    "\n",
    "![Schematic](images/CO-tip-evaluator.png)\n",
    "\n",
    "[1] Alldritt, Benjamin, et al. \"Automated tip functionalization via machine learning in scanning probe microscopy.\" Computer Physics Communications 273 (2022): 108258."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4c5b182-cfd1-47f1-b3eb-731d4b2a7615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataX: (73, 1, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "# data ingestion\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def add_norm_CO(X_):\n",
    "    \"\"\" Normalise input image \"\"\"\n",
    "    sh = X_.shape\n",
    "\n",
    "    for j in range(sh[0]):\n",
    "        mean=np.mean(X_[j,:,:])            \n",
    "        sigma=np.std(X_[j,:,:])          \n",
    "        X_[j,:,:]-= mean\n",
    "        X_[j,:,:]= X_[j,:,:]/ sigma\n",
    "\n",
    "dataX = []\n",
    "file_names = []\n",
    "    \n",
    "for filename in os.listdir(\"examples/images/tip_classification\"):\n",
    "    image_path = os.path.join(\"examples\", \"images\", \"tip_classification\", filename)\n",
    "    image = np.array((Image.open(image_path).resize((16, 16), Image.ANTIALIAS))).astype(np.float32)\n",
    "    dataX.append(np.flipud(image))\n",
    "    file_names.append(filename)\n",
    "dataX = np.expand_dims(np.array(dataX), axis = 1)\n",
    "add_norm_CO(dataX)\n",
    "print (f'dataX: {dataX.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b83395b-71e6-4a25-99b5-18a97bd6266a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 4, 14, 14)         40        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 4, 14, 14)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_1 (Spatial (None, 4, 14, 14)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 12, 12)         148       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 4, 12, 12)         0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_2 (Spatial (None, 4, 12, 12)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 4, 6, 6)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 4, 4)           296       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8, 4, 4)           0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_3 (Spatial (None, 8, 4, 4)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 2, 2)           584       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 8, 2, 2)           0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_4 (Spatial (None, 8, 2, 2)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 2,157\n",
      "Trainable params: 2,157\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "from tensorflow import keras\n",
    "model = keras.models.load_model(\"examples/models/tip_classifier.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e99f9d30-30bc-4e60-9188-f74e1e6c4ef1",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Default AvgPoolingOp only supports NHWC on device type CPU\n\t [[{{node average_pooling2d_1/AvgPool}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1f773739be8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3287\u001b[0m         \u001b[0mfeed_symbols\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_symbols\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m         session != self._session):\n\u001b[0;32m-> 3289\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   3220\u001b[0m       \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3221\u001b[0m     \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3222\u001b[0;31m     \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3223\u001b[0m     \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3224\u001b[0m     \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \"\"\"\n\u001b[1;32m   1488\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1489\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session, callable_options)\u001b[0m\n\u001b[1;32m   1444\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m         self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[0;32m-> 1446\u001b[0;31m             session._session, options_ptr)\n\u001b[0m\u001b[1;32m   1447\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Default AvgPoolingOp only supports NHWC on device type CPU\n\t [[{{node average_pooling2d_1/AvgPool}}]]"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "outputs = model.predict_on_batch(dataX)\n",
    "probs = np.squeeze(np.array(outputs))\n",
    "preds = np.squeeze(np.round(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd44a039-1d08-4262-8810-99a635a7cb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_preds_with_title(dataX,probs,file_names, cmap=cm.gray):\n",
    "    cols = 10\n",
    "    rows = -(-len(file_names) // cols)\n",
    "    fig = plt.figure(figsize=(3.0*cols,3.0*rows))\n",
    "    \n",
    "    for i in range(len(file_names)):\n",
    "        sp = fig.add_subplot(rows,cols,i+1)#, origin=\"lower\"\n",
    "        sp.imshow(dataX[i,0,:,:], cmap = cmap)   \n",
    "        sp.axis('Off')\n",
    "        desc = file_names[i].split(\"-\")[0][:-1]\n",
    "        sp.set_title(f'pred = {probs[i]: 0.3f}\\ntarg = {desc} tip') \n",
    "    \n",
    "    save_name = 'predicted.png'\n",
    "    # plt.savefig('./'+save_name, bbox_inches='tight', dpi=200)\n",
    "    plt.show()\n",
    "    # plt.close()\n",
    "\n",
    "plot_preds_with_title(dataX,probs,file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde94af3-c9e2-42ca-b8cf-2ebc0b975afa",
   "metadata": {},
   "source": [
    "## Tutorial 2\n",
    "\n",
    "### Number recognition\n",
    "Here is a CNN trained to recognize digits from images of handwritten numbers: \n",
    "\n",
    "![digits](images/digits.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3c16c2f-b51d-4ed7-8abc-79635914bf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# get the data and scale it to convenient shapes\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets - test on data not in your training data!\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02cc186e-ce2d-4e63-b444-ee0184110194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# setup the model\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(), #matching array shapes\n",
    "#        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0688ab2-73e4-4e92-9ae2-aa9ff60e6a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/3\n",
      "54000/54000 [==============================] - 13s 236us/sample - loss: 2.2194 - acc: 0.3893 - val_loss: 2.0527 - val_acc: 0.6602\n",
      "Epoch 2/3\n",
      "54000/54000 [==============================] - 11s 197us/sample - loss: 1.9329 - acc: 0.6822 - val_loss: 1.6231 - val_acc: 0.8172\n",
      "Epoch 3/3\n",
      "54000/54000 [==============================] - 10s 193us/sample - loss: 1.4500 - acc: 0.7931 - val_loss: 1.0340 - val_acc: 0.8445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f856a436d10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the CNN\n",
    "batch_size = 10000 # number of images in each batch used for optimisation - balance speed of each epoch against convergence to high accuracy\n",
    "epochs = 3\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "384ce876-97f3-4928-ac70-ca2c5f540944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.0702123623847961\n",
      "Test accuracy: 0.8194\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b53c10-bfce-4d3d-9ae4-7c0d234898a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
