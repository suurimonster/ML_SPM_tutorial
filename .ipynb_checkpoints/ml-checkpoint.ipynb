{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c413a97-9579-4950-9166-1d80256b629b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Machine learning and microscopy\n",
    "=====================\n",
    "\n",
    "Moving to machine learning\n",
    "------------\n",
    "\n",
    "* Machine learning at heart is a software toolset to apply statistical methods to find patterns in data.\n",
    "* “The ability [of machine learning] to automatically identify patterns in data [...] is particularly important when the expert knowledge is incomplete or inaccurate, when the amount of available data is too large to be handled manually, or when there are exceptions to the general cases” [1]\n",
    "\n",
    "![ML](images/machine_learning_joke.jpg)\n",
    "\n",
    "Why NOT use maching learning?\n",
    "------------\n",
    "\n",
    "* It is very tempting to start applying machine learning approaches to everything, but it is critical to step back and see if it makes sense.\n",
    "* Some major reasons NOT to apply an machine learning approach would be:\n",
    "    * There are existing methods that do the job well - there is no point inventing a square wheel.\n",
    "    * You don't have enough data - in general you want to have a ratio of 10:1 between *datapoints* and *features*.\n",
    "    * Your data is poorly organized or heavily biased - if you are not sure about the history and compatibility of your data you will not be able to trust any of your predictions.\n",
    "\n",
    "[1]: Yip KY, Cheng C, Gerstein M. Machine learning and genome annotation: a match meant to be? Genome Biol. 2013;14(5):205."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c56d9-801a-4c7c-95ba-2b35e3701a89",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Recent examples\n",
    "====================\n",
    "\n",
    "Image segmentation\n",
    "---------------------\n",
    "\n",
    "* A very common application of machine learning in microscopy is image segmentation i.e. automatically detecting features in images.\n",
    "* This is generally achieved using Convolutional Neural Networks (CNN) and the approach is the backbone for all the animal classification methods you have likely seen already.\n",
    "* This classification is an example of *supervised learning*, where the training data is labelled by the user. The algorithm then learns this classification and is able to make accurate predictions on new data.\n",
    "* For an example, we often use the U-Net method [1], which is a powerful approach, widely used in many CNN applications. For biomedial imaging, they actually broke some of our inital rules...they had very little data, but used extensive *augmentation* to expand the valid data set. In particular (and general to microscopy of soft systems) they introduced:\n",
    "    * shift and rotation invariance \n",
    "    * gray value variations\n",
    "    * random elastic deformations\n",
    "* It is not perfect (77%), but much better than any alternatives at that time.\n",
    "\n",
    "![segment](images/segment.png)\n",
    "\n",
    "[1]: Ronneberger, O., Fischer, P. & Brox, T. U-Net: Convolutional Networks for Biomedical Image Segmentation. arXiv:1505.04597 [cs] (2015).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe9be1d-3a3b-4229-9fce-95bb8b7c5b84",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Cell classification\n",
    "---------------------\n",
    "\n",
    "* In terms of searching for patterns in images that demonstrate a correlation between a measurement and a hypothesis, machine learning image analysis can also be very powerful.\n",
    "* In the example, they looked at multi-channel AFM data of healthy and cancerous (fixed) cells [1].\n",
    "\n",
    "![cancer_cells](images/cancer_cells.png)\n",
    "\n",
    "* Model was trained on the imaging channels independently and in combination, and the resultant classification accuracy was studied as a function of measured parameters. Very high accuracy was achieved using a combination of channels.\n",
    "\n",
    "![cancer_acc](images/cancer_acc.png)\n",
    "\n",
    "[1]: Prasad, S. et al. Atomic Force Microscopy Detects the Difference in Cancer Cells of Different Neoplastic Aggressiveness via Machine Learning. Advanced NanoBiomed Research 1, 2000116 (2021).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170fe77b-e966-4209-8c64-76da6401f9dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Active learning\n",
    "--------------------\n",
    "\n",
    "* A powerful development in experimental design is the introduction of active learning (also referred to as *Bayesian Optimisation*), where measurements are dynamically chosen to optimise the information obtained.\n",
    "* In piezoresponse force microscopy, this has been used to automate the measurement of spectra at regions of specific interest [1]. The system is first trained on known surface features and spectra.\n",
    "\n",
    "![active_model](images/active_model.png)\n",
    "\n",
    "* Then, when it is linked to the driving software of an SPM, the network itself can make a decision about the most useful place to measure the spectra. These *decisions* can then actually tell us something about the correlation between image features and the nature of the spectra.\n",
    "\n",
    "![active_image](images/active_image.png)\n",
    "\n",
    "[1]: Liu, Y., Kelley, K.P., Vasudevan, R.K. et al. Experimental discovery of structure–property relationships in ferroelectric materials via active learning. Nat Mach Intell 4, 341–350 (2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44bd44f-e736-45cc-b831-29115336af33",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Machine learning methods\n",
    "============================\n",
    "\n",
    "* There are a wide variety of machine learning approaches and as a general rule it is better to use the simplest one that works. In microscopy, since we are often working with images, then Neural Networks (NN) are an obvious place to start, as there has been an enormous effort in recent years to optimise NN tools for image analysis. They also play role in all the examples/tutorials we are considering. Do NOT use them by default...\n",
    "\n",
    "Neural Networks\n",
    "-------------------\n",
    "\n",
    "* The starting point of a neural network is the neuron itself (called a *perceptron* in the context of machine learning) - it takes inputs, applies some operations to them and produces an output [1].\n",
    "\n",
    "![neuron](images/perceptron.svg)\n",
    "\n",
    "* First each input $x_1,x_2$ is multiplied by weights $w_1,w_2$. Then all the weighted inputs are added together with a bias $b$ and an activation function $f$ is applied to the total:\n",
    "\n",
    "$y = f((w_1*x_1)+(w_2*x_2) + b$)\n",
    "\n",
    "* The bias reflects your pre-assumptions in the model i.e. the higher it is, the more you think you know what the answer should be, and the weights determine the strength of connection between this input and the output. These are the *hyper-parameters* that will be optimized.\n",
    "\n",
    "* The activation function turns an unbounded input into a controllable, bounded input. A commonly used example would be a sigmoid function.\n",
    "\n",
    "[1]: Neural Networks from Scratch - https://victorzhou.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91801177-14ab-46b9-afd7-41add675977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "\n",
    "x = np.linspace(-10, 10, 100) \n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "a = sigmoid(x)\n",
    "\n",
    "plt.figure(figsize=(12,7)) # set the figsize\n",
    "plt.plot(x, a) \n",
    "plt.xlabel(\"x\") \n",
    "plt.ylabel(\"sigmoid(x)\")\n",
    "plt.rc('font',size=16) # set the font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e02295-a49d-4e04-83e3-01ff4ae08604",
   "metadata": {},
   "source": [
    "* We can then code a very simple neuron as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ab0ba1-0759-441e-9df8-8b4e8c00016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Neuron: # classes are a powerful aspect of python that allows inherited characteristics\n",
    "  def __init__(self, weights, bias):\n",
    "    self.weights = weights\n",
    "    self.bias = bias\n",
    "\n",
    "  def feedforward(self, inputs): # passing inputs to get an output is known as \"feed forward\"\n",
    "    # Weight inputs, add bias, then use the activation function\n",
    "    total = np.dot(self.weights, inputs) + self.bias\n",
    "    return sigmoid(total)\n",
    "\n",
    "weights = np.array([0, 1]) # w1 = 0, w2 = 1\n",
    "bias = 4                   # b = 4\n",
    "n = Neuron(weights, bias)\n",
    "\n",
    "x = np.array([2, 3])       # x1 = 2, x2 = 3\n",
    "print(n.feedforward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07d2020-dfc2-4a06-8699-b1bca6bcc627",
   "metadata": {},
   "source": [
    "* A neural network is just formed by combing neurons and connecting them.\n",
    "\n",
    "![nn](images/network.svg)\n",
    "\n",
    "* Again, we can make a simple code that implements this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f337e5fa-4886-4af1-838d-1be081ad3c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Neural_Network:\n",
    "  '''\n",
    "  A neural network with:\n",
    "    - 2 inputs\n",
    "    - a hidden layer with 2 neurons (h1, h2)\n",
    "    - an output layer with 1 neuron (o1)\n",
    "  Each neuron has the same weights and bias:\n",
    "    - w = [0, 1]\n",
    "    - b = 0\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    weights = np.array([0, 1])\n",
    "    bias = 0\n",
    "\n",
    "    # The Neuron class here is from the previous code sample\n",
    "    self.h1 = Neuron(weights, bias)\n",
    "    self.h2 = Neuron(weights, bias)\n",
    "    self.o1 = Neuron(weights, bias)\n",
    "\n",
    "  def feedforward(self, x):\n",
    "    out_h1 = self.h1.feedforward(x)\n",
    "    out_h2 = self.h2.feedforward(x)\n",
    "\n",
    "    # The inputs for o1 are the outputs from h1 and h2\n",
    "    out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "\n",
    "    return out_o1\n",
    "\n",
    "network = Neural_Network()\n",
    "x = np.array([2, 3])\n",
    "print(network.feedforward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f73699-34b6-45e5-95c5-5e2d04da31b0",
   "metadata": {},
   "source": [
    "* This appears very abstract, but if we were classifying articles as either from physics or biology based on the number of equations in them, we might assign an output of (0) to physics and (1) to biology, and then train the network on data on equation numbers per article from each field. The training would attempt to minimize the *loss* in the output with respect to the parameters of the model.\n",
    "* The number of hidden layers (the layers between input and output) determine the *depth* of the network and is the source of the term *deep learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b72472-5432-4471-b25c-eeabfaeac95d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Convolutional neural networks\n",
    "----------------------------------\n",
    "\n",
    "* Nearly all image analysis relies on using Convolutional neural networks (CNN) rather than standard neural networks. This is for two main reasons:\n",
    "    * Images are big - imagine building a neural network to process a 224x224 color image - including the 3 color channels (RGB) in the image, that comes out to 224 x 224 x 3 = 150 528 input features. A typical hidden layer in such a network might have 1024 nodes, so we’d have to train 150 528 x 1024 = 150+ million weights for the first layer alone. Our network would be huge and nearly impossible to train.\n",
    "    * Positions can change - if you trained a network to detect dogs, you’d want it to be able to a detect a dog regardless of where it appears in the image. Imagine training a network that works well on a certain dog image, but then feeding it a slightly shifted version of the same image. The dog would not activate the same neurons, so the network would react completely differently.\n",
    "* What we really want to do is learn characteristic features within our training images that would be general to the subject of interest rather than the image itself.\n",
    "\n",
    "*Convolutions*\n",
    "\n",
    "* A CNN basically convolves the image with a filter that will extract features. For example, the Sobel filter:\n",
    "\n",
    "![sobel](images/vertical-sobel.svg)\n",
    "\n",
    "when applied to a simple greyscale image, convolves it into a smaller matrix (using *padding* i.e. adding zeroes at the edges, would allow us to sample more pixels):\n",
    "\n",
    "![convolve](images/convolve-output.gif)\n",
    "\n",
    "at each step it is just performing an element-wise multiplication between the values in the filter and their corresponding values in the image, and then summing up all the element-wise products to get the new output.\n",
    "\n",
    "* What does this actually do?\n",
    "\n",
    "![lenna](images/lenna+vertical.png)\n",
    "\n",
    "The Sobel filter is an edge detector - the image has been transformed into a set of *edge features*.\n",
    "\n",
    "*Pooling*\n",
    "\n",
    "* It is obvious to us (but not to the *machine*) that many features are connected such that many neighbouring pixels have similar values. In the context of the edge detection filter, if we find a strong edge at a location, it is pretty likely that the edge will also be present one pixel away.\n",
    "\n",
    "* To avoid duplicating features, this is normally handled by using *pooling* of neighbouring pixels to there maximum, minimum or average (max-, min- and mean-pooling).\n",
    "\n",
    "![pool](images/pool.gif)\n",
    "\n",
    "*Softmax*\n",
    "\n",
    "* Finally, we need an activation function, just as in the standard NN. A commonly used function for CNNs is Softmax, which has the following form:\n",
    "\n",
    "${Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$\n",
    "\n",
    "* Softmax produces a value from 0 to 1, as for the Sigmoid, but also the outputs add up to 1, so it has the form of a probability - this is not needed, but it allows for the use of *cross entropy loss*, which includes a component of prediction confidence. So the very basic form of a CNN is the following:\n",
    "\n",
    "![cnn](images/cnn.svg)\n",
    "\n",
    "* Here we start with a 28x28 image and convolve it with 8 filters. Using a maximum pooling with size 2 reduces the image to 13x13. There are 10 Softmax nodes, which would assume we are trying to classify our data into 10 classes. The final output from Softmax for a given image would be the node with the highest probability i.e. the class prediction.\n",
    "\n",
    "*Training*\n",
    "\n",
    "* Before we actually let you play around with training your own network, a few comments on the process itself. It generally consists of two phases:\n",
    "    * A forward phase, where the input is passed completely through the network - key values are stored in preparation for the backwards pass.\n",
    "    * A backward phase, where gradients are backpropagated (backprop) and weights are updated. During this phase, each layer will receive the gradient of loss with respect to its outputs and return the gradient of loss with respect to its inputs - this establishes the dependence of loss on our parameters and allows for optimisation (with a standard optimiser like ADAM).\n",
    "* The basic code for CNN training would have this form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf0afe7-2185-4f2c-b652-b99a4042c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed forward\n",
    "out = conv.forward(image)\n",
    "out = pool.forward(out)\n",
    "out = softmax.forward(out)\n",
    "\n",
    "# Calculate initial gradient\n",
    "gradient = np.zeros(10)\n",
    "# ...\n",
    "\n",
    "# Backprop\n",
    "gradient = softmax.backprop(gradient)\n",
    "gradient = pool.backprop(gradient)\n",
    "gradient = conv.backprop(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad94fb9-baea-4d09-b102-cf90bca2eee7",
   "metadata": {},
   "source": [
    "* The core of training is in the `conv.backprop` step, as any changes in the filter weights is effectively changing the nature of the filters and the type of features associated with them.\n",
    "* When really applying these methods in a scientific context, it is much more efficient to use highly optimised models, such as `tensorflow`. As in the tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b14550e-ae4d-4bca-89f5-c06e4b86c22a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Tutorials\n",
    "=====================\n",
    "\n",
    "\n",
    "1. Make sure you can run the simple examples in the lecture and understand how they build up into a *real* network.\n",
    "2. Iris flower data set - a neural network trained on the properties of plants.\n",
    "3. Number recognition - a convolutional neural network trained to recognize handwritten digits from 1-9.\n",
    "4. CO functionalised AFM tip classification - a convolutional neural network trained to classify good and bad tips in AFM imaging.\n",
    "5. Generative adversarial networks (GANs) - use a GAN to make realistic handwriting predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36654ec4-9786-427e-809a-757153f87391",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Iris flower data set\n",
    "\n",
    "The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper *\"The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis\"*.\n",
    "\n",
    "The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n",
    "\n",
    "We can import the needed libraries, and then we can load the iris data.\n",
    "\n",
    "![iris](images/Iris_versicolor_3.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a032139a-120e-4cba-9829-2a134136ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "with open(\"examples/data/iris.pkl\", \"rb\") as fio:\n",
    "    iris = pickle.load(fio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a01611-eafc-4128-9c2a-ef1149b0abb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see the data\n",
    "from pprint import pprint\n",
    "pprint(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f8c62-51a5-4cae-9223-a151578a386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare input data\n",
    "x = iris[\"data\"]\n",
    "input_shape = (4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d9fe57-1dbb-450a-a68e-0ab2911dca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare output data\n",
    "target = iris[\"target\"]\n",
    "num_classes = 3\n",
    "# one-hot representation\n",
    "y = keras.utils.to_categorical(target, num_classes)\n",
    "\n",
    "print(f\"targets: {target[:5]}\\none-hot rep:{y[:5, :]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5561ad8-9c15-4b2d-90f7-68da24dc4ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "shuffle_indices = np.random.permutation(np.arange(len(target)))\n",
    "x = x[shuffle_indices]\n",
    "y = y[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ddad2a-760c-435d-ac5e-2821621ee84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_train split 80-20\n",
    "split = int(len(target) * 0.8)\n",
    "x_train, x_test, y_train, y_test = x[:split, :], x[split:, :], y[:split, :], y[split:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4508bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d2652-30b4-4065-b167-5d91821f3678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Dense(3, activation=\"relu\"),\n",
    "        layers.Dense(num_classes, activation=\"softmax\")\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0783db-8763-42d9-b92f-99680ede9961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "batch_size = 5 # number of images in each batch used for optimisation - balance speed of each epoch against convergence to high accuracy\n",
    "epochs = 200\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd386c26-91fc-4bcf-ace6-0758140cb844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea5a00f-cc24-4f23-b22f-b713771128d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Number recognition\n",
    "Here is a CNN trained to recognize digits from images of handwritten numbers: \n",
    "\n",
    "![digits](images/digits.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c16c2f-b51d-4ed7-8abc-79635914bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data and scale it to convenient shapes\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets - test on data not in your training data!\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc186e-ce2d-4e63-b444-ee0184110194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the model\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(), #matching array shapes\n",
    "#        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0688ab2-73e4-4e92-9ae2-aa9ff60e6a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the CNN\n",
    "batch_size = 10000 # number of images in each batch used for optimisation - balance speed of each epoch against convergence to high accuracy\n",
    "epochs = 3\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ce876-97f3-4928-ac70-ca2c5f540944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71612c5-2c32-45c4-90a1-217f53143185",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## CO functionalised AFM tip classification\n",
    "An automated solution for carbon monoxide functionalization which combines machine learning descriptors with automated software control of the tip preparation process. \n",
    "\n",
    "![Schematic](images/CO-tip-evaluator.png)\n",
    "\n",
    "[1] Alldritt, Benjamin, et al. \"Automated tip functionalization via machine learning in scanning probe microscopy.\" Computer Physics Communications 273 (2022): 108258."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c5b182-cfd1-47f1-b3eb-731d4b2a7615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data ingestion\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def add_norm_CO(X_):\n",
    "    \"\"\" Normalise input image \"\"\"\n",
    "    sh = X_.shape\n",
    "\n",
    "    for j in range(sh[0]):\n",
    "        mean=np.mean(X_[j,:,:])            \n",
    "        sigma=np.std(X_[j,:,:])          \n",
    "        X_[j,:,:]-= mean\n",
    "        X_[j,:,:]= X_[j,:,:]/ sigma\n",
    "\n",
    "dataX = []\n",
    "file_names = []\n",
    "    \n",
    "for filename in os.listdir(\"examples/images/tip_classification\"):\n",
    "    image_path = os.path.join(\"examples\", \"images\", \"tip_classification\", filename)\n",
    "    image = np.array((Image.open(image_path).resize((16, 16), Image.ANTIALIAS))).astype(np.float32)\n",
    "    dataX.append(np.flipud(image))\n",
    "    file_names.append(filename)\n",
    "dataX = np.expand_dims(np.array(dataX), axis = 1)\n",
    "add_norm_CO(dataX)\n",
    "print (f'dataX: {dataX.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b83395b-71e6-4a25-99b5-18a97bd6266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "from tensorflow import keras\n",
    "model = keras.models.load_model(\"examples/models/tip_classifier.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99f9d30-30bc-4e60-9188-f74e1e6c4ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "outputs = model.predict_on_batch(dataX)\n",
    "probs = np.squeeze(np.array(outputs))\n",
    "preds = np.squeeze(np.round(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd44a039-1d08-4262-8810-99a635a7cb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_preds_with_title(dataX,probs,file_names, cmap=cm.gray):\n",
    "    cols = 10\n",
    "    rows = -(-len(file_names) // cols)\n",
    "    fig = plt.figure(figsize=(3.0*cols,3.0*rows))\n",
    "    \n",
    "    for i in range(len(file_names)):\n",
    "        sp = fig.add_subplot(rows,cols,i+1)#, origin=\"lower\"\n",
    "        sp.imshow(dataX[i,0,:,:], cmap = cmap)   \n",
    "        sp.axis('Off')\n",
    "        desc = file_names[i].split(\"-\")[0][:-1]\n",
    "        sp.set_title(f'pred = {probs[i]: 0.3f}\\ntarg = {desc} tip') \n",
    "    \n",
    "    save_name = 'predicted.png'\n",
    "    # plt.savefig('./'+save_name, bbox_inches='tight', dpi=200)\n",
    "    plt.show()\n",
    "    # plt.close()\n",
    "\n",
    "plot_preds_with_title(dataX,probs,file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc519c94",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Generative adversarial networks (GANs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c8ce04",
   "metadata": {},
   "source": [
    "* Using generative models we can learn the underlying distribution of a given dataset: after training, we can thus run these models in inference mode and generate new, realistic data points.\n",
    "\n",
    "* An interesting category of generative models is given by Generative Adversarial Networks (https://arxiv.org/abs/1406.2661). \n",
    "\n",
    "* These architectures are based on a game theoretical approach, where two networks compete with each other and are simultaneously trained, until reaching an equilibrium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad69ba29",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "Let's suppose we want to generate images which could realistically belong to the MNIST handwritten dataset. A \"Generator\" network will be fed with vectors drawn from a random distribution, and its goal during the training will be to generate increasingly realistic images.\n",
    "\n",
    "\n",
    "More specifically, the goal of the Generator will be maximixing the probability that *another* network, a \"Discriminator\" (which is trained to tell apart real from fake data) will classify its own, generated output as real.\n",
    "\n",
    "![gen](images/gen.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2b3ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569612f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will be generating 28x28 images\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4ba801",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = keras.Sequential()\n",
    "\n",
    "generator.add(Dense(256, input_dim=latent_dim))\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "generator.add(Dense(512))\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "generator.add(Dense(1024))\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "generator.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "generator.add(Reshape(img_shape))\n",
    "\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb364c",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "The Discriminator network will receive as input both true (belonging to the dataset) and fake (output from the Generator) images, and will be trained to be as good as possible in classifying them correctly. \n",
    "\n",
    "The task will become more difficult with eath iteration, because at the same time the Generator is being trained to \"trick\" the Discriminator. \n",
    "\n",
    "![discr1](images/discr1.gif)\n",
    "![discr2](images/discr2.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f8915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = keras.Sequential()\n",
    "\n",
    "discriminator.add(Flatten(input_shape=img_shape))\n",
    "discriminator.add(Dense(512))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(BatchNormalization(momentum=0.8))\n",
    "discriminator.add(Dense(256))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090786f",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The architecture can be summarized as follows:\n",
    "\n",
    "<img src=\"images/gan.png\" width=\"850\">\n",
    "\n",
    "* What we expect during the training is that the Generator's loss will increase at the start, and then decrease, reaching a plateau. \n",
    "\n",
    "* Conversely, the Discriminator's loss will initially decrease almost to zero, and then gradually increase until plateuing as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe0cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to plot losses and generated samples\n",
    "\n",
    "def plot_loss(losses):\n",
    "    d_loss = [v[0] for v in losses[\"D\"]]\n",
    "    g_loss = [v for v in losses[\"G\"]]\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(d_loss, label=\"Discriminator loss\")\n",
    "    plt.plot(g_loss, label=\"Generator loss\")\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_generated(n_ex=10, dim=(1, 10), figsize=(12, 2)):\n",
    "    noise = np.random.normal(0, 1, size=(n_ex, latent_dim))\n",
    "    generated_images = generator.predict(noise)\n",
    "    generated_images = generated_images.reshape(n_ex, 28, 28)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e29ef1",
   "metadata": {},
   "source": [
    "Let's compile the models to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db752a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# compile the discriminator\n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# ---------------------\n",
    "# Combined Model\n",
    "# ---------------------\n",
    "\n",
    "# While the discriminator can be trained indipendently,\n",
    "# the generator needs to receive the discriminator's loss,\n",
    "# to be able to adapt itself to it. \n",
    "\n",
    "# Thus we need to combine generator and discriminator in a single model,\n",
    "# in which however the discriminator will be frozen (not trainable)\n",
    "\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The generator takes noise as input and generates imgs\n",
    "z = Input(shape=(latent_dim,))\n",
    "img = generator(z)\n",
    "\n",
    "# The discriminator takes generated images as input and determines validity\n",
    "validity = discriminator(img)\n",
    "\n",
    "# The combined model (stacked generator and discriminator) trains\n",
    "# the generator to fool the discriminator\n",
    "gan = Model(z, validity)\n",
    "# compile the combined model\n",
    "gan.compile(loss='binary_crossentropy', optimizer=optimizer) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e971c59",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce6289",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, _), (_, _) = mnist.load_data()\n",
    "X_train = X_train / 127.5 - 1.   # Rescale from -1 to 1\n",
    "X_train = np.expand_dims(X_train, axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6236d4e6",
   "metadata": {},
   "source": [
    "Choose training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ec4faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many batches of data will we go through in the training\n",
    "iterations = 15000\n",
    "# Each with a size:\n",
    "batch_size = 128\n",
    "# How often we want to plot generated images and plot losses\n",
    "plt_frq = 100\n",
    "\n",
    "# A dictionary to store the losses\n",
    "losses = {\"D\":[], \"G\":[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99a92ad",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78907f5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adversarial ground truths \n",
    "valid = np.ones((batch_size, 1)) \n",
    "fake = np.zeros((batch_size, 1))\n",
    "\n",
    "for iteration in range(iterations):\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Discriminator\n",
    "    # ---------------------\n",
    "\n",
    "    # Select a random batch of images\n",
    "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "    imgs = X_train[idx]\n",
    "\n",
    "    # Create noise vectors for the generator\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "    # Generate a batch of new images\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Train the discriminator\n",
    "    discriminator.trainable = True\n",
    "    \n",
    "    # \"train_on_batch\" takes an input vector and a target vector: the real images are \n",
    "    # compared to the target outcome (a vector of 1s, i.e. all classified real), and viceversa \n",
    "    # for the generated images (compared to a vector of 0s i.e. all classified fake)\n",
    "    d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
    "    d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
    "    \n",
    "    # The final discriminator loss is given in equal parts by how good the discriminator is in \n",
    "    # correctly classifying both real and fake images\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Generator\n",
    "    # ---------------------\n",
    "\n",
    "    discriminator.trainable = False\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "    # The generator loss decreases if the percentage of fake images classified as \n",
    "    # real by the discriminator increases\n",
    "    g_loss = gan.train_on_batch(noise, valid)\n",
    "\n",
    "    # Store losses \n",
    "    losses[\"D\"].append(d_loss)\n",
    "    losses[\"G\"].append(g_loss)\n",
    "\n",
    "    # Update the plots\n",
    "    if iteration == 1 or iteration%plt_frq == 0:\n",
    "        # Plot the progress\n",
    "        print (\"%d [D loss: %f] [G loss: %f]\" % (iteration, d_loss[0], g_loss))\n",
    "        plot_generated()\n",
    "        plot_loss(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
